{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define paths\n",
    "train_folder_path = r\"C:\\Emotion detection deep learning\\images\\train\"\n",
    "\n",
    "# Emotion categories (assume you have folders named as emotions)\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_images(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for emotion in emotions:\n",
    "        emotion_folder = os.path.join(folder_path, emotion)\n",
    "        if not os.path.exists(emotion_folder):\n",
    "            print(f\"Folder not found: {emotion_folder}\")\n",
    "            continue\n",
    "        for img_name in os.listdir(emotion_folder):\n",
    "            img_path = os.path.join(emotion_folder, img_name)\n",
    "            if not img_name.endswith(('jpg', 'jpeg', 'png', 'gif')):\n",
    "                continue\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # Read in grayscale\n",
    "            img = cv2.resize(img, (48, 48)) # Resize to 48x48 pixels\n",
    "            images.append(img)\n",
    "            labels.append(emotion)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load images and labels\n",
    "images, labels = load_images(train_folder_path)\n",
    "\n",
    "if len(images) == 0:\n",
    "    raise ValueError(\"No images found in the specified folders. Please check the paths and folder names.\")\n",
    "\n",
    "# Normalize pixel values\n",
    "images = images / 255.0\n",
    "\n",
    "# Encode labels\n",
    "label_to_index = {emotion: index for index, emotion in enumerate(emotions)}\n",
    "labels = np.array([label_to_index[label] for label in labels])\n",
    "labels = to_categorical(labels, num_classes=len(emotions))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # First convolutional layer\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Second convolutional layer\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Third convolutional layer\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(emotions), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabh\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 154ms/step - accuracy: 0.3519 - loss: 1.6704 - val_accuracy: 0.5336 - val_loss: 1.2306\n",
      "Epoch 2/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 162ms/step - accuracy: 0.3764 - loss: 1.5935 - val_accuracy: 0.5398 - val_loss: 1.2177\n",
      "Epoch 3/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 223ms/step - accuracy: 0.4022 - loss: 1.5340 - val_accuracy: 0.5466 - val_loss: 1.2039\n",
      "Epoch 4/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 142ms/step - accuracy: 0.4108 - loss: 1.5207 - val_accuracy: 0.5410 - val_loss: 1.2033\n",
      "Epoch 5/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4077 - loss: 1.5103 - val_accuracy: 0.5329 - val_loss: 1.2105\n",
      "Epoch 6/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 173ms/step - accuracy: 0.4104 - loss: 1.5033 - val_accuracy: 0.5356 - val_loss: 1.2072\n",
      "Epoch 7/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 86ms/step - accuracy: 0.4133 - loss: 1.5102 - val_accuracy: 0.5330 - val_loss: 1.2176\n",
      "Epoch 8/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 58ms/step - accuracy: 0.4222 - loss: 1.4930 - val_accuracy: 0.5435 - val_loss: 1.2075\n",
      "Epoch 9/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4239 - loss: 1.4778 - val_accuracy: 0.5206 - val_loss: 1.2264\n",
      "Epoch 10/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 77ms/step - accuracy: 0.4299 - loss: 1.4779 - val_accuracy: 0.5400 - val_loss: 1.2132\n",
      "Epoch 11/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 152ms/step - accuracy: 0.4302 - loss: 1.4708 - val_accuracy: 0.5424 - val_loss: 1.2051\n",
      "Epoch 12/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 152ms/step - accuracy: 0.4451 - loss: 1.4518 - val_accuracy: 0.5457 - val_loss: 1.2050\n",
      "Epoch 13/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 156ms/step - accuracy: 0.4454 - loss: 1.4506 - val_accuracy: 0.5426 - val_loss: 1.1976\n",
      "Epoch 14/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 189ms/step - accuracy: 0.4406 - loss: 1.4452 - val_accuracy: 0.5514 - val_loss: 1.1737\n",
      "Epoch 15/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 145ms/step - accuracy: 0.4455 - loss: 1.4434 - val_accuracy: 0.5492 - val_loss: 1.1892\n",
      "Epoch 16/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 174ms/step - accuracy: 0.4519 - loss: 1.4201 - val_accuracy: 0.5488 - val_loss: 1.1833\n",
      "Epoch 17/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 160ms/step - accuracy: 0.4435 - loss: 1.4364 - val_accuracy: 0.5473 - val_loss: 1.1908\n",
      "Epoch 18/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4566 - loss: 1.4123 - val_accuracy: 0.5445 - val_loss: 1.1872\n",
      "Epoch 19/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.4510 - loss: 1.4206 - val_accuracy: 0.5483 - val_loss: 1.1699\n",
      "Epoch 20/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4582 - loss: 1.4222 - val_accuracy: 0.5440 - val_loss: 1.1807\n",
      "Epoch 21/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 74ms/step - accuracy: 0.4525 - loss: 1.4165 - val_accuracy: 0.5549 - val_loss: 1.1662\n",
      "Epoch 22/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 75ms/step - accuracy: 0.4544 - loss: 1.4297 - val_accuracy: 0.5516 - val_loss: 1.1737\n",
      "Epoch 23/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.4597 - loss: 1.4040 - val_accuracy: 0.5473 - val_loss: 1.1963\n",
      "Epoch 24/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.4599 - loss: 1.4112 - val_accuracy: 0.5589 - val_loss: 1.1747\n",
      "Epoch 25/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4692 - loss: 1.3926 - val_accuracy: 0.5554 - val_loss: 1.1702\n",
      "Epoch 26/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4647 - loss: 1.4048 - val_accuracy: 0.5495 - val_loss: 1.1768\n",
      "Epoch 27/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.4666 - loss: 1.3899 - val_accuracy: 0.5526 - val_loss: 1.1765\n",
      "Epoch 28/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4601 - loss: 1.4027 - val_accuracy: 0.5578 - val_loss: 1.1584\n",
      "Epoch 29/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 77ms/step - accuracy: 0.4721 - loss: 1.3725 - val_accuracy: 0.5618 - val_loss: 1.1609\n",
      "Epoch 30/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - accuracy: 0.4767 - loss: 1.3764 - val_accuracy: 0.5563 - val_loss: 1.1537\n",
      "Epoch 31/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.4655 - loss: 1.3856 - val_accuracy: 0.5549 - val_loss: 1.1706\n",
      "Epoch 32/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4694 - loss: 1.3901 - val_accuracy: 0.5530 - val_loss: 1.1735\n",
      "Epoch 33/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 77ms/step - accuracy: 0.4765 - loss: 1.3846 - val_accuracy: 0.5598 - val_loss: 1.1590\n",
      "Epoch 34/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 74ms/step - accuracy: 0.4672 - loss: 1.3852 - val_accuracy: 0.5655 - val_loss: 1.1418\n",
      "Epoch 35/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.4709 - loss: 1.3758 - val_accuracy: 0.5585 - val_loss: 1.1515\n",
      "Epoch 36/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.4792 - loss: 1.3672 - val_accuracy: 0.5636 - val_loss: 1.1592\n",
      "Epoch 37/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.4796 - loss: 1.3656 - val_accuracy: 0.5594 - val_loss: 1.1539\n",
      "Epoch 38/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.4749 - loss: 1.3716 - val_accuracy: 0.5506 - val_loss: 1.1624\n",
      "Epoch 39/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.4736 - loss: 1.3852 - val_accuracy: 0.5608 - val_loss: 1.1459\n",
      "Epoch 40/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - accuracy: 0.4856 - loss: 1.3640 - val_accuracy: 0.5537 - val_loss: 1.1708\n",
      "Epoch 41/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 70ms/step - accuracy: 0.4746 - loss: 1.3616 - val_accuracy: 0.5691 - val_loss: 1.1447\n",
      "Epoch 42/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 79ms/step - accuracy: 0.4810 - loss: 1.3626 - val_accuracy: 0.5632 - val_loss: 1.1476\n",
      "Epoch 43/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.4785 - loss: 1.3635 - val_accuracy: 0.5691 - val_loss: 1.1322\n",
      "Epoch 44/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 73ms/step - accuracy: 0.4792 - loss: 1.3667 - val_accuracy: 0.5710 - val_loss: 1.1371\n",
      "Epoch 45/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.4813 - loss: 1.3476 - val_accuracy: 0.5627 - val_loss: 1.1489\n",
      "Epoch 46/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 76ms/step - accuracy: 0.4879 - loss: 1.3603 - val_accuracy: 0.5625 - val_loss: 1.1429\n",
      "Epoch 47/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 78ms/step - accuracy: 0.4758 - loss: 1.3608 - val_accuracy: 0.5620 - val_loss: 1.1503\n",
      "Epoch 48/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 71ms/step - accuracy: 0.4871 - loss: 1.3526 - val_accuracy: 0.5712 - val_loss: 1.1410\n",
      "Epoch 49/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 78ms/step - accuracy: 0.4923 - loss: 1.3450 - val_accuracy: 0.5740 - val_loss: 1.1461\n",
      "Epoch 50/50\n",
      "\u001b[1m361/361\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 73ms/step - accuracy: 0.4838 - loss: 1.3513 - val_accuracy: 0.5722 - val_loss: 1.1403\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('emotion_detection_model.keras', save_best_only=True)\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Fit the model using augmented data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=64),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5720 - loss: 1.1358\n",
      "Validation Loss: 1.1321589946746826\n",
      "Validation Accuracy: 0.5691240429878235\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation data\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020F6E251BC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "The predicted emotion is: happy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('emotion_detection_model.keras')\n",
    "\n",
    "# Function to predict emotion from an image\n",
    "def predict_emotion(img_path):\n",
    "    # Load and preprocess the image\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (48, 48))\n",
    "    img = img / 255.0\n",
    "    img = np.reshape(img, (1, 48, 48, 1))\n",
    "    \n",
    "    # Predict the emotion\n",
    "    predictions = model.predict(img)\n",
    "    max_index = np.argmax(predictions[0])\n",
    "    \n",
    "    # Emotion categories\n",
    "    emotions = ['anger', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    \n",
    "    # Return the predicted emotion\n",
    "    return emotions[max_index]\n",
    "\n",
    "# Example usage\n",
    "img_path = r\"C:\\Users\\prabh\\Downloads\\WhatsApp Image 2024-12-12 at 12.28.25 AM.jpeg\"\n",
    "predicted_emotion = predict_emotion(img_path)\n",
    "print(f\"The predicted emotion is: {predicted_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
